# DocumentaciÃ³n TÃ©cnica del Proyecto ETL y Dashboard

## ğŸŒ DescripciÃ³n General

Este proyecto implementa un pipeline ETL completo para la integraciÃ³n de datos desde varias fuentes de videojuegos:

* **Twitch API** (streams y juegos)
* **HowLongToBeat** (duraciÃ³n de juegos)
* **PokÃ©mon Showdown (vÃ­a PokÃ©API)** (informaciÃ³n de PokÃ©mon)

Los datos se extraen, transforman y cargan en una base de datos **PostgreSQL**. AdemÃ¡s, se incluyen dashboards interactivos para analizar los datos desde diferentes perspectivas.

---

## âš™ï¸ TecnologÃ­as Utilizadas

* **Python** con PySpark
* **Docker** (para PostgreSQL y ETL)
* **PostgreSQL** como base de datos destino
* **Streamlit** (opcional para dashboards)
* **Langchain + FastAPI** (opcional si se desea extender a chatbot)

---

## ğŸ“š Estructura del Proyecto

```
.
â”œâ”€â”€ extract/                 # ExtracciÃ³n de datos desde APIs
â”œâ”€â”€ transform/              # Limpieza y transformaciones
â”œâ”€â”€ carga/                   # Carga a base de datos
â”œâ”€â”€ data/                   # Archivos Parquet generados
â”œâ”€â”€ db/
â”‚   â””â”€â”€ create_database_structure.sql  # Script para crear estructura en PostgreSQL
â”œâ”€â”€ dashboard/              # CÃ³digo de visualizaciÃ³n Streamlit (opcional)
â”œâ”€â”€ docker-compose.yml      # OrquestaciÃ³n de servicios
â”œâ”€â”€ requirements.txt
â””â”€â”€ main.py                 # Orquestador del pipeline ETL
```

---

## âš¡ï¸ EjecuciÃ³n RÃ¡pida (Quickstart)

### 1. Clonar el repositorio

```bash
git clone <REPO_URL>
cd nombre-del-proyecto
```

### 2. Levantar servicios con Docker

```bash
docker-compose up --build
```

Esto lanzarÃ¡:

* Una base de datos PostgreSQL expuesta en `localhost:5432`
* El contenedor de ETL que ejecutarÃ¡ todo el pipeline al iniciarse

---

## ğŸ“ CÃ³mo Funciona el Pipeline

### FASE 1: ExtracciÃ³n

Los scripts del mÃ³dulo `extract/` obtienen los datos de APIs y los convierten en DataFrames de Spark.

### FASE 2: TransformaciÃ³n

En `transform/clean.py` se limpian y homogeneizan los datos:

* ConversiÃ³n de tipos
* EliminaciÃ³n de duplicados
* NormalizaciÃ³n de campos

### FASE 3: Carga a BD

El mÃ³dulo `load/to_db.py` se encarga de:

* Eliminar columnas incompatibles con JDBC
* Insertar datos en tablas PostgreSQL con logs de carga

---

## ğŸ”„ Ciclo de Vida del Pipeline

Cada ejecuciÃ³n:

1. Extrae datos frescos de todas las fuentes
2. Los guarda como Parquet (copia limpia)
3. Inserta los datos transformados en la base de datos
4. Registra los logs de carga en la tabla `dw_log_cargas`

---

## ğŸ“Š Dashboard y Visualizaciones (opcional con Streamlit)

Puedes lanzar el dashboard si has implementado `dashboard/` con Streamlit:

```bash
streamlit run dashboard/main.py
```

### Ejemplos:

* Twitch: juegos mÃ¡s populares, nÃºmero de streams por categorÃ­a
* HowLongToBeat: juegos mÃ¡s largos, comparaciÃ³n entre modos
* PokÃ©mon: top de estadÃ­sticas, distribuciÃ³n por tipo
* Logs: nÃºmero de cargas por fuente, Ãºltima carga, errores

---

## âœ‰ï¸ Contacto / Autores

* Autor: Alan McLure AlarcÃ³n
* Curso: EspecializaciÃ³n en IA y Big Data
* Fecha: Junio 2025
